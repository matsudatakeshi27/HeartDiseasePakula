{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f37971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fdffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_files = ['cle_train.csv','cle_test.csv','hun_train.csv','hun_test.csv','swi_train.csv','swi_test.csv','vir_train.csv','vir_test.csv']\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for file in data_files:\n",
    "    data = pd.read_csv('../' + file)\n",
    "    \n",
    "    X = data.iloc[:, :-1]\n",
    "    Y = data.iloc[:, -1]\n",
    "    \n",
    "    Y_binary = Y.apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Extract the name from the file path\n",
    "    name = file.split('.')[0]\n",
    "    \n",
    "    # Store the dataset components in a dictionary\n",
    "    datasets[name] = {'X': X, 'Y': Y, 'Y_binary': Y_binary}\n",
    "\n",
    "# Unpack the dictionary values in a loop\n",
    "variables = ['cle', 'hun', 'swi', 'vir']\n",
    "train_test = ['train', 'test']\n",
    "\n",
    "for var in variables:\n",
    "    for tt in train_test:\n",
    "        X, Y, Y_binary = datasets[f'{var}_{tt}'].values()\n",
    "        globals()[f'{var}_X_{tt}'] = X\n",
    "        globals()[f'{var}_Y_{tt}'] = Y\n",
    "        globals()[f'{var}_Y_{tt}_binary'] = Y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8171119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([cle_X_test,hun_X_test,swi_X_test,vir_X_test])\n",
    "y_test = pd.concat([cle_Y_test_binary,hun_Y_test_binary,swi_Y_test_binary,vir_Y_test_binary])\n",
    "\n",
    "X_train = pd.concat([cle_X_train,hun_X_train,swi_X_train,vir_X_train])\n",
    "y_train = pd.concat([cle_Y_train_binary,hun_Y_train_binary,swi_Y_train_binary,vir_Y_train_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients():\n",
    "    cle_zip = list(zip(cle_X_train.values,cle_Y_train_binary))\n",
    "    hun_zip = list(zip(hun_X_train.values,hun_Y_train_binary))\n",
    "    vir_zip = list(zip(vir_X_train.values,vir_Y_train_binary))\n",
    "    swi_zip = list(zip(swi_X_train.values,swi_Y_train_binary))\n",
    "    \n",
    "    shards = [cle_zip, hun_zip, vir_zip,swi_zip]\n",
    "    client_names = [\"client_1\",\"client_2\",\"client_3\",\"client_4\"]\n",
    "    dic = {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "    return dic\n",
    "\n",
    "\n",
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(22,), activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    length = len(y_test)\n",
    "    Y_test = tf.reshape(Y_test,(length,1))\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), Y_test)\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94fcc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\votri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 0 | global_acc: 67.241% | global_loss: 0.6864417791366577\n",
      "comm_round: 1 | global_acc: 70.690% | global_loss: 0.6812266111373901\n",
      "comm_round: 2 | global_acc: 70.345% | global_loss: 0.676790714263916\n",
      "comm_round: 3 | global_acc: 73.793% | global_loss: 0.6724902987480164\n",
      "comm_round: 4 | global_acc: 75.172% | global_loss: 0.6677857637405396\n",
      "comm_round: 5 | global_acc: 75.517% | global_loss: 0.6630774736404419\n",
      "comm_round: 6 | global_acc: 76.552% | global_loss: 0.6582602262496948\n",
      "comm_round: 7 | global_acc: 77.241% | global_loss: 0.6529224514961243\n",
      "comm_round: 8 | global_acc: 78.276% | global_loss: 0.6481412649154663\n",
      "comm_round: 9 | global_acc: 78.966% | global_loss: 0.6428323984146118\n",
      "comm_round: 10 | global_acc: 78.621% | global_loss: 0.6370717287063599\n",
      "comm_round: 11 | global_acc: 78.966% | global_loss: 0.6318754553794861\n",
      "comm_round: 12 | global_acc: 78.966% | global_loss: 0.6267182230949402\n",
      "comm_round: 13 | global_acc: 79.310% | global_loss: 0.6215376257896423\n",
      "comm_round: 14 | global_acc: 79.655% | global_loss: 0.6165189743041992\n",
      "comm_round: 15 | global_acc: 79.655% | global_loss: 0.6113572120666504\n",
      "comm_round: 16 | global_acc: 80.345% | global_loss: 0.6069417595863342\n",
      "comm_round: 17 | global_acc: 81.724% | global_loss: 0.6033019423484802\n",
      "comm_round: 18 | global_acc: 81.034% | global_loss: 0.5988625288009644\n",
      "comm_round: 19 | global_acc: 81.379% | global_loss: 0.5949589014053345\n",
      "comm_round: 20 | global_acc: 82.414% | global_loss: 0.5917046666145325\n",
      "comm_round: 21 | global_acc: 82.069% | global_loss: 0.5888078212738037\n",
      "comm_round: 22 | global_acc: 82.414% | global_loss: 0.5863699316978455\n",
      "comm_round: 23 | global_acc: 82.414% | global_loss: 0.5836799144744873\n",
      "comm_round: 24 | global_acc: 82.414% | global_loss: 0.5810022950172424\n",
      "comm_round: 25 | global_acc: 82.759% | global_loss: 0.5793805718421936\n",
      "comm_round: 26 | global_acc: 82.759% | global_loss: 0.5775435566902161\n",
      "comm_round: 27 | global_acc: 82.759% | global_loss: 0.5750617980957031\n",
      "comm_round: 28 | global_acc: 82.759% | global_loss: 0.5734402537345886\n",
      "comm_round: 29 | global_acc: 82.759% | global_loss: 0.5716534852981567\n",
      "comm_round: 30 | global_acc: 82.414% | global_loss: 0.5699617862701416\n",
      "comm_round: 31 | global_acc: 82.414% | global_loss: 0.5694599747657776\n",
      "comm_round: 32 | global_acc: 82.414% | global_loss: 0.568300724029541\n",
      "comm_round: 33 | global_acc: 82.414% | global_loss: 0.5668504238128662\n",
      "comm_round: 34 | global_acc: 82.414% | global_loss: 0.5660281181335449\n",
      "comm_round: 35 | global_acc: 83.103% | global_loss: 0.566272497177124\n",
      "comm_round: 36 | global_acc: 82.414% | global_loss: 0.5651040077209473\n",
      "comm_round: 37 | global_acc: 82.069% | global_loss: 0.5642856955528259\n",
      "comm_round: 38 | global_acc: 82.069% | global_loss: 0.563470721244812\n",
      "comm_round: 39 | global_acc: 82.414% | global_loss: 0.5623338222503662\n",
      "comm_round: 40 | global_acc: 82.759% | global_loss: 0.561474084854126\n",
      "comm_round: 41 | global_acc: 83.103% | global_loss: 0.5609613656997681\n",
      "comm_round: 42 | global_acc: 83.103% | global_loss: 0.5600534677505493\n",
      "comm_round: 43 | global_acc: 82.759% | global_loss: 0.5596330761909485\n",
      "comm_round: 44 | global_acc: 82.759% | global_loss: 0.5590767860412598\n",
      "comm_round: 45 | global_acc: 83.103% | global_loss: 0.5584647059440613\n",
      "comm_round: 46 | global_acc: 83.103% | global_loss: 0.5580641627311707\n",
      "comm_round: 47 | global_acc: 83.103% | global_loss: 0.5570892095565796\n",
      "comm_round: 48 | global_acc: 82.759% | global_loss: 0.5571858882904053\n",
      "comm_round: 49 | global_acc: 82.414% | global_loss: 0.5563218593597412\n",
      "comm_round: 50 | global_acc: 82.414% | global_loss: 0.555752694606781\n",
      "comm_round: 51 | global_acc: 82.414% | global_loss: 0.5552200675010681\n",
      "comm_round: 52 | global_acc: 82.414% | global_loss: 0.5550817251205444\n",
      "comm_round: 53 | global_acc: 82.414% | global_loss: 0.5548133850097656\n",
      "comm_round: 54 | global_acc: 82.414% | global_loss: 0.5536290407180786\n",
      "comm_round: 55 | global_acc: 82.414% | global_loss: 0.5537307858467102\n",
      "comm_round: 56 | global_acc: 82.414% | global_loss: 0.5537513494491577\n",
      "comm_round: 57 | global_acc: 82.414% | global_loss: 0.5531890392303467\n",
      "comm_round: 58 | global_acc: 82.414% | global_loss: 0.5527725219726562\n",
      "comm_round: 59 | global_acc: 82.069% | global_loss: 0.5519819855690002\n",
      "comm_round: 60 | global_acc: 82.069% | global_loss: 0.552247166633606\n",
      "comm_round: 61 | global_acc: 82.069% | global_loss: 0.5518267750740051\n",
      "comm_round: 62 | global_acc: 82.414% | global_loss: 0.5520172715187073\n",
      "comm_round: 63 | global_acc: 82.069% | global_loss: 0.5517910718917847\n",
      "comm_round: 64 | global_acc: 82.069% | global_loss: 0.551089882850647\n",
      "comm_round: 65 | global_acc: 82.069% | global_loss: 0.5512002110481262\n",
      "comm_round: 66 | global_acc: 82.069% | global_loss: 0.5506341457366943\n",
      "comm_round: 67 | global_acc: 82.069% | global_loss: 0.5500198602676392\n",
      "comm_round: 68 | global_acc: 82.069% | global_loss: 0.5492401719093323\n",
      "comm_round: 69 | global_acc: 82.069% | global_loss: 0.5500878691673279\n",
      "comm_round: 70 | global_acc: 82.069% | global_loss: 0.5497740507125854\n",
      "comm_round: 71 | global_acc: 82.069% | global_loss: 0.5492487549781799\n",
      "comm_round: 72 | global_acc: 82.069% | global_loss: 0.5495250225067139\n",
      "comm_round: 73 | global_acc: 82.069% | global_loss: 0.5490046739578247\n",
      "comm_round: 74 | global_acc: 82.069% | global_loss: 0.5487959980964661\n",
      "comm_round: 75 | global_acc: 82.069% | global_loss: 0.5485527515411377\n",
      "comm_round: 76 | global_acc: 82.069% | global_loss: 0.5483160018920898\n",
      "comm_round: 77 | global_acc: 82.069% | global_loss: 0.548190176486969\n",
      "comm_round: 78 | global_acc: 82.069% | global_loss: 0.5481225848197937\n",
      "comm_round: 79 | global_acc: 82.069% | global_loss: 0.5475660562515259\n",
      "comm_round: 80 | global_acc: 82.069% | global_loss: 0.5477643013000488\n",
      "comm_round: 81 | global_acc: 82.414% | global_loss: 0.5474123358726501\n",
      "comm_round: 82 | global_acc: 82.414% | global_loss: 0.5470542907714844\n",
      "comm_round: 83 | global_acc: 82.069% | global_loss: 0.5475159883499146\n",
      "comm_round: 84 | global_acc: 82.069% | global_loss: 0.5464423894882202\n",
      "comm_round: 85 | global_acc: 82.069% | global_loss: 0.5463336110115051\n",
      "comm_round: 86 | global_acc: 82.069% | global_loss: 0.5458894371986389\n",
      "comm_round: 87 | global_acc: 82.414% | global_loss: 0.5464248657226562\n",
      "comm_round: 88 | global_acc: 82.414% | global_loss: 0.5455678105354309\n",
      "comm_round: 89 | global_acc: 82.759% | global_loss: 0.5458180904388428\n",
      "comm_round: 90 | global_acc: 82.414% | global_loss: 0.5450929999351501\n",
      "comm_round: 91 | global_acc: 82.414% | global_loss: 0.5457401871681213\n",
      "comm_round: 92 | global_acc: 82.414% | global_loss: 0.5449479818344116\n",
      "comm_round: 93 | global_acc: 82.759% | global_loss: 0.5446817278862\n",
      "comm_round: 94 | global_acc: 82.414% | global_loss: 0.5440673232078552\n",
      "comm_round: 95 | global_acc: 82.069% | global_loss: 0.5439092516899109\n",
      "comm_round: 96 | global_acc: 82.414% | global_loss: 0.5438605546951294\n",
      "comm_round: 97 | global_acc: 82.069% | global_loss: 0.5439227223396301\n",
      "comm_round: 98 | global_acc: 82.414% | global_loss: 0.5432181358337402\n",
      "comm_round: 99 | global_acc: 82.414% | global_loss: 0.5429766178131104\n",
      "comm_round: 1 | global_acc: 85.172% | global_loss: 0.5335047245025635\n"
     ]
    }
   ],
   "source": [
    "#create clients\n",
    "clients = create_clients()\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "comms_round = 100\n",
    "    \n",
    "#create optimizer\n",
    "lr = 0.01 \n",
    "loss='sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.SGD(lr=lr, decay=lr / comms_round, momentum=0.9) \n",
    "\n",
    "#initialize global model\n",
    "smlp_global = DNN()\n",
    "global_model = smlp_global.build(22, 2)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = DNN()\n",
    "        local_model = smlp_local.build(22, 2)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(250)\n",
    "        smlp_SGD = DNN()\n",
    "        SGD_model = smlp_SGD.build(22, 2) \n",
    "\n",
    "        SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b13ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions = np.argmax(SGD_model.predict(X_test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc551a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,  19],\n",
       "       [ 24, 142]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_predictions, Y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a83e0bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8468    0.8140    0.8300       129\n",
      "           1     0.8554    0.8820    0.8685       161\n",
      "\n",
      "    accuracy                         0.8517       290\n",
      "   macro avg     0.8511    0.8480    0.8493       290\n",
      "weighted avg     0.8516    0.8517    0.8514       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56495b0e",
   "metadata": {},
   "source": [
    "# Testing on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec02651a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  4]\n",
      " [ 5 36]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.9231    0.9143        52\n",
      "           1     0.9000    0.8780    0.8889        41\n",
      "\n",
      "    accuracy                         0.9032        93\n",
      "   macro avg     0.9028    0.9006    0.9016        93\n",
      "weighted avg     0.9032    0.9032    0.9031        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_cle = np.argmax(SGD_model.predict(cle_X_test),axis = 1)\n",
    "cm_cle = confusion_matrix(Y_cle, cle_Y_test_binary)\n",
    "print(cm_cle)\n",
    "print(classification_report(Y_cle, cle_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b7cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 36, 55, 61, 62, 70, 77, 80]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_cle, cle_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d4cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  4]\n",
      " [14 38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3000    0.6000    0.4000        10\n",
      "           1     0.9048    0.7308    0.8085        52\n",
      "\n",
      "    accuracy                         0.7097        62\n",
      "   macro avg     0.6024    0.6654    0.6043        62\n",
      "weighted avg     0.8072    0.7097    0.7426        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_vir = np.argmax(SGD_model.predict(vir_X_test),axis = 1)\n",
    "cm_vir = confusion_matrix(Y_vir, vir_Y_test_binary)\n",
    "print(cm_vir)\n",
    "print(classification_report(Y_vir, vir_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625cfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 13, 14, 15, 19, 27, 33, 35, 41, 44, 45, 54, 55, 57, 59, 60]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_vir, vir_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c031520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  6]\n",
      " [ 4 34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9259    0.8929    0.9091        56\n",
      "           1     0.8500    0.8947    0.8718        38\n",
      "\n",
      "    accuracy                         0.8936        94\n",
      "   macro avg     0.8880    0.8938    0.8904        94\n",
      "weighted avg     0.8952    0.8936    0.8940        94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_hun = np.argmax(SGD_model.predict(hun_X_test),axis = 1)\n",
    "cm_hun = confusion_matrix(Y_hun, hun_Y_test_binary)\n",
    "print(cm_hun)\n",
    "print(classification_report(Y_hun, hun_Y_test_binary,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec7ffbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 44, 61, 65, 66, 74, 77, 81, 86, 90]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_hun, hun_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e837211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5]\n",
      " [ 1 34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.1667    0.2500         6\n",
      "           1     0.8718    0.9714    0.9189        35\n",
      "\n",
      "    accuracy                         0.8537        41\n",
      "   macro avg     0.6859    0.5690    0.5845        41\n",
      "weighted avg     0.8174    0.8537    0.8210        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_swi = np.argmax(SGD_model.predict(swi_X_test),axis = 1)\n",
    "cm_swi = confusion_matrix(Y_swi, swi_Y_test_binary)\n",
    "print(cm_swi)\n",
    "print(classification_report(Y_swi, swi_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805b6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 8, 11, 19, 24, 26]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_swi, swi_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

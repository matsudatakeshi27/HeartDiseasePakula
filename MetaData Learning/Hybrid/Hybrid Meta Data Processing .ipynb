{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a140d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470c377",
   "metadata": {},
   "source": [
    "# Hybrid metadata creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dfb1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wholeData(path_train,path_test):\n",
    "    #Split the data\n",
    "    Train = pd.read_csv(path_train)\n",
    "    Test = pd.read_csv(path_test)\n",
    "    \n",
    "    X_train = Train.iloc[:,:-1]\n",
    "    Y_train = Train.iloc[:,-1]\n",
    "    \n",
    "    X_test = Test.iloc[:,:-1]\n",
    "    Y_test = Test.iloc[:,-1]\n",
    "    \n",
    "    X_entire = pd.concat([X_train,X_test])\n",
    "    Y_entire = pd.concat([Y_train,Y_test])\n",
    "    return X_entire,Y_entire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c2ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl(path_train, path_test):\n",
    "    #Split the data\n",
    "    Train = pd.read_csv(path_train)\n",
    "    Test = pd.read_csv(path_test)\n",
    "    \n",
    "    X_train = Train.iloc[:,:-1]\n",
    "    Y_train = Train.iloc[:,-1]\n",
    "\n",
    "    X_test = Test.iloc[:,:-1]\n",
    "    Y_test = Test.iloc[:,-1]\n",
    "\n",
    "    #binarize the target\n",
    "    Y_train_binary = Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "    Y_test_binary = Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "        \n",
    "    list_dnn = [\"cle_dnn.h5\", \"hun_dnn.h5\"]\n",
    "    output = pd.DataFrame()\n",
    "    for i in list_dnn:\n",
    "        model = tf.keras.models.load_model('../Models/DNN_only/' + i, compile=False)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model2 = Sequential()\n",
    "        model2.add(Dense(64, input_shape=(22,), activation='relu'))\n",
    "        model2.add(Dense(128, activation='relu'))\n",
    "        model2.add(Dense(256, activation='relu'))\n",
    "        model2.add(Dense(16, activation='sigmoid'))\n",
    "\n",
    "        model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        X_entire,labels = wholeData(path_train,path_test)\n",
    "\n",
    "        Y_predictions = model2.predict(X_entire)\n",
    "        Y_predictions.shape\n",
    "\n",
    "        temp = pd.DataFrame(Y_predictions)\n",
    "        output = pd.concat([output,temp],axis = 1)\n",
    "        \n",
    "    list_cnn = [\"vir_cnn.h5\"]\n",
    "    for j in list_cnn:\n",
    "        model = tf.keras.models.load_model('../Models/CNN_only/' + j, compile=False)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        model2 = Sequential()\n",
    "        model2.add(Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(22,1)))\n",
    "        model2.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model2.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "        model2.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model2.add(Flatten())\n",
    "        model2.add(Dense(16, activation='relu'))\n",
    "\n",
    "        model2.set_weights(model.get_weights()[:-2]) \n",
    "\n",
    "        model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        X_entire,labels = wholeData(path_train,path_test)\n",
    "\n",
    "        Y_predictions = model2.predict(X_entire)\n",
    "        Y_predictions.shape\n",
    "\n",
    "        temp = pd.DataFrame(Y_predictions)\n",
    "        output = pd.concat([output,temp],axis = 1)\n",
    "    \n",
    "    labels = labels.reset_index()\n",
    "    labels = labels.drop(columns = ['index'])\n",
    "\n",
    "    output = pd.concat([output,labels],axis = 1)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3109a2f",
   "metadata": {},
   "source": [
    "# Cleveland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4869202",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../TrainTestData/cle_train.csv'\n",
    "path_test = '../TrainTestData/cle_test.csv'\n",
    "\n",
    "output = tl(path_train, path_test)\n",
    "\n",
    "output.to_csv(\"../Metadata Learning/\" + \"cle_metadata_hybrid.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbda735f",
   "metadata": {},
   "source": [
    "# Virginia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804f93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../TrainTestData/vir_train.csv'\n",
    "path_test = '../TrainTestData/vir_test.csv'\n",
    "\n",
    "output = tl(path_train, path_test)\n",
    "\n",
    "output.to_csv(\"../Metadata Learning/\" + \"vir_metadata_hybrid.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52570f93",
   "metadata": {},
   "source": [
    "# Hungarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26723604",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../TrainTestData/hun_train.csv'\n",
    "path_test = '../TrainTestData/hun_test.csv'\n",
    "\n",
    "output = tl(path_train, path_test)\n",
    "\n",
    "output.to_csv(\"../Metadata Learning/\" + \"hun_metadata_hybrid.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c5ab4",
   "metadata": {},
   "source": [
    "# Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f326ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../TrainTestData/swi_train.csv'\n",
    "path_test = '../TrainTestData/swi_test.csv'\n",
    "\n",
    "output = tl(path_train, path_test)\n",
    "\n",
    "output.to_csv(\"../Metadata Learning/\" + \"swi_metadata_hybrid.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27776f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

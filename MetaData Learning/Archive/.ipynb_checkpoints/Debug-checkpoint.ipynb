{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b95d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1ee0e",
   "metadata": {},
   "source": [
    "# DNN metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3360ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_train=pd.read_csv('cle_metadata_dnn_train.csv')\n",
    "cle_test=pd.read_csv('cle_metadata_dnn_test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e589bed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7.3</th>\n",
       "      <th>8.3</th>\n",
       "      <th>9.3</th>\n",
       "      <th>10.3</th>\n",
       "      <th>11.3</th>\n",
       "      <th>12.3</th>\n",
       "      <th>13.3</th>\n",
       "      <th>14.3</th>\n",
       "      <th>15.3</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.482027</td>\n",
       "      <td>0.499887</td>\n",
       "      <td>0.530806</td>\n",
       "      <td>0.538419</td>\n",
       "      <td>0.491713</td>\n",
       "      <td>0.513956</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>0.541717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506952</td>\n",
       "      <td>0.529668</td>\n",
       "      <td>0.515936</td>\n",
       "      <td>0.512529</td>\n",
       "      <td>0.464183</td>\n",
       "      <td>0.451921</td>\n",
       "      <td>0.443915</td>\n",
       "      <td>0.494462</td>\n",
       "      <td>0.505076</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454859</td>\n",
       "      <td>0.529616</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.552197</td>\n",
       "      <td>0.496266</td>\n",
       "      <td>0.510415</td>\n",
       "      <td>0.484137</td>\n",
       "      <td>0.507131</td>\n",
       "      <td>0.496896</td>\n",
       "      <td>0.540834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527008</td>\n",
       "      <td>0.547635</td>\n",
       "      <td>0.518892</td>\n",
       "      <td>0.498704</td>\n",
       "      <td>0.482539</td>\n",
       "      <td>0.431991</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.484820</td>\n",
       "      <td>0.495692</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463892</td>\n",
       "      <td>0.528533</td>\n",
       "      <td>0.529637</td>\n",
       "      <td>0.528451</td>\n",
       "      <td>0.506440</td>\n",
       "      <td>0.525559</td>\n",
       "      <td>0.475964</td>\n",
       "      <td>0.484750</td>\n",
       "      <td>0.524651</td>\n",
       "      <td>0.529517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497336</td>\n",
       "      <td>0.537428</td>\n",
       "      <td>0.500808</td>\n",
       "      <td>0.509069</td>\n",
       "      <td>0.493877</td>\n",
       "      <td>0.466631</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.492282</td>\n",
       "      <td>0.490346</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.502789</td>\n",
       "      <td>0.547796</td>\n",
       "      <td>0.545979</td>\n",
       "      <td>0.500581</td>\n",
       "      <td>0.501580</td>\n",
       "      <td>0.497734</td>\n",
       "      <td>0.493490</td>\n",
       "      <td>0.481571</td>\n",
       "      <td>0.520887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501154</td>\n",
       "      <td>0.519787</td>\n",
       "      <td>0.473487</td>\n",
       "      <td>0.509875</td>\n",
       "      <td>0.493117</td>\n",
       "      <td>0.438414</td>\n",
       "      <td>0.439596</td>\n",
       "      <td>0.496251</td>\n",
       "      <td>0.512598</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487966</td>\n",
       "      <td>0.502613</td>\n",
       "      <td>0.530262</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>0.498579</td>\n",
       "      <td>0.490217</td>\n",
       "      <td>0.480382</td>\n",
       "      <td>0.510005</td>\n",
       "      <td>0.498182</td>\n",
       "      <td>0.530342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498314</td>\n",
       "      <td>0.532062</td>\n",
       "      <td>0.471401</td>\n",
       "      <td>0.476258</td>\n",
       "      <td>0.532875</td>\n",
       "      <td>0.461315</td>\n",
       "      <td>0.455472</td>\n",
       "      <td>0.480313</td>\n",
       "      <td>0.495240</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.466115</td>\n",
       "      <td>0.535069</td>\n",
       "      <td>0.528014</td>\n",
       "      <td>0.543189</td>\n",
       "      <td>0.502908</td>\n",
       "      <td>0.497021</td>\n",
       "      <td>0.490430</td>\n",
       "      <td>0.508133</td>\n",
       "      <td>0.494720</td>\n",
       "      <td>0.515179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521451</td>\n",
       "      <td>0.534069</td>\n",
       "      <td>0.497134</td>\n",
       "      <td>0.487809</td>\n",
       "      <td>0.479236</td>\n",
       "      <td>0.426793</td>\n",
       "      <td>0.432726</td>\n",
       "      <td>0.483189</td>\n",
       "      <td>0.495828</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.453091</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.516624</td>\n",
       "      <td>0.514484</td>\n",
       "      <td>0.498208</td>\n",
       "      <td>0.498271</td>\n",
       "      <td>0.470482</td>\n",
       "      <td>0.482329</td>\n",
       "      <td>0.487317</td>\n",
       "      <td>0.548821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505642</td>\n",
       "      <td>0.513401</td>\n",
       "      <td>0.480499</td>\n",
       "      <td>0.511517</td>\n",
       "      <td>0.526581</td>\n",
       "      <td>0.448357</td>\n",
       "      <td>0.462745</td>\n",
       "      <td>0.502072</td>\n",
       "      <td>0.489047</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.482236</td>\n",
       "      <td>0.531854</td>\n",
       "      <td>0.511274</td>\n",
       "      <td>0.533771</td>\n",
       "      <td>0.522290</td>\n",
       "      <td>0.515086</td>\n",
       "      <td>0.500259</td>\n",
       "      <td>0.493102</td>\n",
       "      <td>0.494401</td>\n",
       "      <td>0.528609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530208</td>\n",
       "      <td>0.510428</td>\n",
       "      <td>0.497206</td>\n",
       "      <td>0.481540</td>\n",
       "      <td>0.498352</td>\n",
       "      <td>0.444947</td>\n",
       "      <td>0.461585</td>\n",
       "      <td>0.508294</td>\n",
       "      <td>0.477561</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.465662</td>\n",
       "      <td>0.532874</td>\n",
       "      <td>0.532976</td>\n",
       "      <td>0.511328</td>\n",
       "      <td>0.512178</td>\n",
       "      <td>0.527493</td>\n",
       "      <td>0.450629</td>\n",
       "      <td>0.471850</td>\n",
       "      <td>0.503037</td>\n",
       "      <td>0.530560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506905</td>\n",
       "      <td>0.525362</td>\n",
       "      <td>0.505196</td>\n",
       "      <td>0.499030</td>\n",
       "      <td>0.506256</td>\n",
       "      <td>0.465933</td>\n",
       "      <td>0.451788</td>\n",
       "      <td>0.504041</td>\n",
       "      <td>0.487671</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.472999</td>\n",
       "      <td>0.491385</td>\n",
       "      <td>0.545966</td>\n",
       "      <td>0.540674</td>\n",
       "      <td>0.491898</td>\n",
       "      <td>0.496575</td>\n",
       "      <td>0.494038</td>\n",
       "      <td>0.504556</td>\n",
       "      <td>0.497916</td>\n",
       "      <td>0.510383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492659</td>\n",
       "      <td>0.531702</td>\n",
       "      <td>0.500553</td>\n",
       "      <td>0.501605</td>\n",
       "      <td>0.466254</td>\n",
       "      <td>0.439234</td>\n",
       "      <td>0.441449</td>\n",
       "      <td>0.514051</td>\n",
       "      <td>0.516591</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.482027  0.499887  0.530806  0.538419  0.491713  0.513956  0.498010   \n",
       "1    0.454859  0.529616  0.526296  0.552197  0.496266  0.510415  0.484137   \n",
       "2    0.463892  0.528533  0.529637  0.528451  0.506440  0.525559  0.475964   \n",
       "3    0.487112  0.502789  0.547796  0.545979  0.500581  0.501580  0.497734   \n",
       "4    0.487966  0.502613  0.530262  0.509978  0.498579  0.490217  0.480382   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "198  0.466115  0.535069  0.528014  0.543189  0.502908  0.497021  0.490430   \n",
       "199  0.453091  0.514880  0.516624  0.514484  0.498208  0.498271  0.470482   \n",
       "200  0.482236  0.531854  0.511274  0.533771  0.522290  0.515086  0.500259   \n",
       "201  0.465662  0.532874  0.532976  0.511328  0.512178  0.527493  0.450629   \n",
       "202  0.472999  0.491385  0.545966  0.540674  0.491898  0.496575  0.494038   \n",
       "\n",
       "            7         8         9  ...       7.3       8.3       9.3  \\\n",
       "0    0.488720  0.489319  0.541717  ...  0.506952  0.529668  0.515936   \n",
       "1    0.507131  0.496896  0.540834  ...  0.527008  0.547635  0.518892   \n",
       "2    0.484750  0.524651  0.529517  ...  0.497336  0.537428  0.500808   \n",
       "3    0.493490  0.481571  0.520887  ...  0.501154  0.519787  0.473487   \n",
       "4    0.510005  0.498182  0.530342  ...  0.498314  0.532062  0.471401   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "198  0.508133  0.494720  0.515179  ...  0.521451  0.534069  0.497134   \n",
       "199  0.482329  0.487317  0.548821  ...  0.505642  0.513401  0.480499   \n",
       "200  0.493102  0.494401  0.528609  ...  0.530208  0.510428  0.497206   \n",
       "201  0.471850  0.503037  0.530560  ...  0.506905  0.525362  0.505196   \n",
       "202  0.504556  0.497916  0.510383  ...  0.492659  0.531702  0.500553   \n",
       "\n",
       "         10.3      11.3      12.3      13.3      14.3      15.3   num  \n",
       "0    0.512529  0.464183  0.451921  0.443915  0.494462  0.505076  0.25  \n",
       "1    0.498704  0.482539  0.431991  0.433700  0.484820  0.495692  0.25  \n",
       "2    0.509069  0.493877  0.466631  0.448718  0.492282  0.490346  0.00  \n",
       "3    0.509875  0.493117  0.438414  0.439596  0.496251  0.512598  0.50  \n",
       "4    0.476258  0.532875  0.461315  0.455472  0.480313  0.495240  0.25  \n",
       "..        ...       ...       ...       ...       ...       ...   ...  \n",
       "198  0.487809  0.479236  0.426793  0.432726  0.483189  0.495828  0.75  \n",
       "199  0.511517  0.526581  0.448357  0.462745  0.502072  0.489047  0.50  \n",
       "200  0.481540  0.498352  0.444947  0.461585  0.508294  0.477561  0.00  \n",
       "201  0.499030  0.506256  0.465933  0.451788  0.504041  0.487671  0.00  \n",
       "202  0.501605  0.466254  0.439234  0.441449  0.514051  0.516591  0.25  \n",
       "\n",
       "[203 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cle_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0b8fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_train=pd.read_csv('vir_metadata_dnn_train.csv' )\n",
    "vir_test=pd.read_csv('vir_metadata_dnn_test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e92518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hun_train=pd.read_csv('hun_metadata_dnn_train.csv' )\n",
    "hun_test=pd.read_csv('hun_metadata_dnn_test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fe3741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "swi_train=pd.read_csv('swi_metadata_dnn_train.csv' )\n",
    "swi_test=pd.read_csv('swi_metadata_dnn_test.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4e16fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.concat([cle_train,vir_train,hun_train])\n",
    "Test = pd.concat([cle_test,vir_test,hun_test,swi_test,swi_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63a819bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = Train.iloc[:,:-1]\n",
    "X_test = Test.iloc[:,:-1]\n",
    "\n",
    "y_train = Train.iloc[:,-1]\n",
    "y_test = Test.iloc[:,-1]\n",
    "\n",
    "Y_train_binary = y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "Y_test_binary = y_test.apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43269e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6.2</th>\n",
       "      <th>7.2</th>\n",
       "      <th>8.2</th>\n",
       "      <th>9.2</th>\n",
       "      <th>10.2</th>\n",
       "      <th>11.2</th>\n",
       "      <th>12.2</th>\n",
       "      <th>13.2</th>\n",
       "      <th>14.2</th>\n",
       "      <th>15.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.482027</td>\n",
       "      <td>0.499887</td>\n",
       "      <td>0.530806</td>\n",
       "      <td>0.538419</td>\n",
       "      <td>0.491713</td>\n",
       "      <td>0.513956</td>\n",
       "      <td>0.498010</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>0.489319</td>\n",
       "      <td>0.541717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456004</td>\n",
       "      <td>0.549547</td>\n",
       "      <td>0.518709</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.519562</td>\n",
       "      <td>0.484019</td>\n",
       "      <td>0.472015</td>\n",
       "      <td>0.508887</td>\n",
       "      <td>0.467478</td>\n",
       "      <td>0.486530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454859</td>\n",
       "      <td>0.529616</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.552197</td>\n",
       "      <td>0.496266</td>\n",
       "      <td>0.510415</td>\n",
       "      <td>0.484137</td>\n",
       "      <td>0.507131</td>\n",
       "      <td>0.496896</td>\n",
       "      <td>0.540834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477837</td>\n",
       "      <td>0.488366</td>\n",
       "      <td>0.493662</td>\n",
       "      <td>0.497254</td>\n",
       "      <td>0.565816</td>\n",
       "      <td>0.497960</td>\n",
       "      <td>0.456738</td>\n",
       "      <td>0.500947</td>\n",
       "      <td>0.496931</td>\n",
       "      <td>0.517806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463892</td>\n",
       "      <td>0.528533</td>\n",
       "      <td>0.529637</td>\n",
       "      <td>0.528451</td>\n",
       "      <td>0.506440</td>\n",
       "      <td>0.525559</td>\n",
       "      <td>0.475964</td>\n",
       "      <td>0.484750</td>\n",
       "      <td>0.524651</td>\n",
       "      <td>0.529517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476820</td>\n",
       "      <td>0.499960</td>\n",
       "      <td>0.477971</td>\n",
       "      <td>0.530497</td>\n",
       "      <td>0.613168</td>\n",
       "      <td>0.473111</td>\n",
       "      <td>0.434871</td>\n",
       "      <td>0.508940</td>\n",
       "      <td>0.505146</td>\n",
       "      <td>0.497114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.502789</td>\n",
       "      <td>0.547796</td>\n",
       "      <td>0.545979</td>\n",
       "      <td>0.500581</td>\n",
       "      <td>0.501580</td>\n",
       "      <td>0.497734</td>\n",
       "      <td>0.493490</td>\n",
       "      <td>0.481571</td>\n",
       "      <td>0.520887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467420</td>\n",
       "      <td>0.523350</td>\n",
       "      <td>0.508465</td>\n",
       "      <td>0.497361</td>\n",
       "      <td>0.530505</td>\n",
       "      <td>0.484081</td>\n",
       "      <td>0.486503</td>\n",
       "      <td>0.484897</td>\n",
       "      <td>0.458825</td>\n",
       "      <td>0.480811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.487966</td>\n",
       "      <td>0.502613</td>\n",
       "      <td>0.530262</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>0.498579</td>\n",
       "      <td>0.490217</td>\n",
       "      <td>0.480382</td>\n",
       "      <td>0.510005</td>\n",
       "      <td>0.498182</td>\n",
       "      <td>0.530342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497544</td>\n",
       "      <td>0.513097</td>\n",
       "      <td>0.488849</td>\n",
       "      <td>0.503623</td>\n",
       "      <td>0.556923</td>\n",
       "      <td>0.495072</td>\n",
       "      <td>0.435286</td>\n",
       "      <td>0.503057</td>\n",
       "      <td>0.478725</td>\n",
       "      <td>0.497577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.499090</td>\n",
       "      <td>0.485432</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.501777</td>\n",
       "      <td>0.497235</td>\n",
       "      <td>0.539960</td>\n",
       "      <td>0.507036</td>\n",
       "      <td>0.509651</td>\n",
       "      <td>0.487092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498627</td>\n",
       "      <td>0.495420</td>\n",
       "      <td>0.542838</td>\n",
       "      <td>0.521122</td>\n",
       "      <td>0.546799</td>\n",
       "      <td>0.499795</td>\n",
       "      <td>0.508507</td>\n",
       "      <td>0.519300</td>\n",
       "      <td>0.540199</td>\n",
       "      <td>0.509817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.487457</td>\n",
       "      <td>0.497015</td>\n",
       "      <td>0.483772</td>\n",
       "      <td>0.485285</td>\n",
       "      <td>0.508247</td>\n",
       "      <td>0.509226</td>\n",
       "      <td>0.528066</td>\n",
       "      <td>0.481849</td>\n",
       "      <td>0.509548</td>\n",
       "      <td>0.491745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506569</td>\n",
       "      <td>0.498009</td>\n",
       "      <td>0.525461</td>\n",
       "      <td>0.509321</td>\n",
       "      <td>0.529209</td>\n",
       "      <td>0.500230</td>\n",
       "      <td>0.519957</td>\n",
       "      <td>0.517985</td>\n",
       "      <td>0.528162</td>\n",
       "      <td>0.487911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.485392</td>\n",
       "      <td>0.501150</td>\n",
       "      <td>0.487716</td>\n",
       "      <td>0.501907</td>\n",
       "      <td>0.500393</td>\n",
       "      <td>0.502232</td>\n",
       "      <td>0.531877</td>\n",
       "      <td>0.506334</td>\n",
       "      <td>0.497089</td>\n",
       "      <td>0.507085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483471</td>\n",
       "      <td>0.509448</td>\n",
       "      <td>0.537361</td>\n",
       "      <td>0.524609</td>\n",
       "      <td>0.535848</td>\n",
       "      <td>0.487393</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.514915</td>\n",
       "      <td>0.511081</td>\n",
       "      <td>0.502285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.498913</td>\n",
       "      <td>0.476920</td>\n",
       "      <td>0.501231</td>\n",
       "      <td>0.512111</td>\n",
       "      <td>0.504014</td>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.500242</td>\n",
       "      <td>0.501795</td>\n",
       "      <td>0.510305</td>\n",
       "      <td>0.502916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.494905</td>\n",
       "      <td>0.524905</td>\n",
       "      <td>0.506340</td>\n",
       "      <td>0.513383</td>\n",
       "      <td>0.489446</td>\n",
       "      <td>0.509562</td>\n",
       "      <td>0.514878</td>\n",
       "      <td>0.532703</td>\n",
       "      <td>0.509106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.483704</td>\n",
       "      <td>0.504237</td>\n",
       "      <td>0.479324</td>\n",
       "      <td>0.476369</td>\n",
       "      <td>0.498507</td>\n",
       "      <td>0.506080</td>\n",
       "      <td>0.533774</td>\n",
       "      <td>0.483517</td>\n",
       "      <td>0.510733</td>\n",
       "      <td>0.496863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516407</td>\n",
       "      <td>0.483216</td>\n",
       "      <td>0.524806</td>\n",
       "      <td>0.516051</td>\n",
       "      <td>0.522063</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.507316</td>\n",
       "      <td>0.535838</td>\n",
       "      <td>0.534950</td>\n",
       "      <td>0.464300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>533 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.482027  0.499887  0.530806  0.538419  0.491713  0.513956  0.498010   \n",
       "1    0.454859  0.529616  0.526296  0.552197  0.496266  0.510415  0.484137   \n",
       "2    0.463892  0.528533  0.529637  0.528451  0.506440  0.525559  0.475964   \n",
       "3    0.487112  0.502789  0.547796  0.545979  0.500581  0.501580  0.497734   \n",
       "4    0.487966  0.502613  0.530262  0.509978  0.498579  0.490217  0.480382   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "191  0.483588  0.499090  0.485432  0.493333  0.501777  0.497235  0.539960   \n",
       "192  0.487457  0.497015  0.483772  0.485285  0.508247  0.509226  0.528066   \n",
       "193  0.485392  0.501150  0.487716  0.501907  0.500393  0.502232  0.531877   \n",
       "194  0.498913  0.476920  0.501231  0.512111  0.504014  0.500044  0.500242   \n",
       "195  0.483704  0.504237  0.479324  0.476369  0.498507  0.506080  0.533774   \n",
       "\n",
       "            7         8         9  ...       6.2       7.2       8.2  \\\n",
       "0    0.488720  0.489319  0.541717  ...  0.456004  0.549547  0.518709   \n",
       "1    0.507131  0.496896  0.540834  ...  0.477837  0.488366  0.493662   \n",
       "2    0.484750  0.524651  0.529517  ...  0.476820  0.499960  0.477971   \n",
       "3    0.493490  0.481571  0.520887  ...  0.467420  0.523350  0.508465   \n",
       "4    0.510005  0.498182  0.530342  ...  0.497544  0.513097  0.488849   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "191  0.507036  0.509651  0.487092  ...  0.498627  0.495420  0.542838   \n",
       "192  0.481849  0.509548  0.491745  ...  0.506569  0.498009  0.525461   \n",
       "193  0.506334  0.497089  0.507085  ...  0.483471  0.509448  0.537361   \n",
       "194  0.501795  0.510305  0.502916  ...  0.499994  0.494905  0.524905   \n",
       "195  0.483517  0.510733  0.496863  ...  0.516407  0.483216  0.524806   \n",
       "\n",
       "          9.2      10.2      11.2      12.2      13.2      14.2      15.2  \n",
       "0    0.493687  0.519562  0.484019  0.472015  0.508887  0.467478  0.486530  \n",
       "1    0.497254  0.565816  0.497960  0.456738  0.500947  0.496931  0.517806  \n",
       "2    0.530497  0.613168  0.473111  0.434871  0.508940  0.505146  0.497114  \n",
       "3    0.497361  0.530505  0.484081  0.486503  0.484897  0.458825  0.480811  \n",
       "4    0.503623  0.556923  0.495072  0.435286  0.503057  0.478725  0.497577  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "191  0.521122  0.546799  0.499795  0.508507  0.519300  0.540199  0.509817  \n",
       "192  0.509321  0.529209  0.500230  0.519957  0.517985  0.528162  0.487911  \n",
       "193  0.524609  0.535848  0.487393  0.505649  0.514915  0.511081  0.502285  \n",
       "194  0.506340  0.513383  0.489446  0.509562  0.514878  0.532703  0.509106  \n",
       "195  0.516051  0.522063  0.520908  0.507316  0.535838  0.534950  0.464300  \n",
       "\n",
       "[533 rows x 48 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.iloc[:,:-16]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db3d3c91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6.2</th>\n",
       "      <th>7.2</th>\n",
       "      <th>8.2</th>\n",
       "      <th>9.2</th>\n",
       "      <th>10.2</th>\n",
       "      <th>11.2</th>\n",
       "      <th>12.2</th>\n",
       "      <th>13.2</th>\n",
       "      <th>14.2</th>\n",
       "      <th>15.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428787</td>\n",
       "      <td>0.525325</td>\n",
       "      <td>0.542139</td>\n",
       "      <td>0.478648</td>\n",
       "      <td>0.506824</td>\n",
       "      <td>0.490862</td>\n",
       "      <td>0.482278</td>\n",
       "      <td>0.505213</td>\n",
       "      <td>0.516506</td>\n",
       "      <td>0.520210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469042</td>\n",
       "      <td>0.494732</td>\n",
       "      <td>0.483015</td>\n",
       "      <td>0.492952</td>\n",
       "      <td>0.539282</td>\n",
       "      <td>0.433638</td>\n",
       "      <td>0.487028</td>\n",
       "      <td>0.496327</td>\n",
       "      <td>0.451854</td>\n",
       "      <td>0.460499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.444054</td>\n",
       "      <td>0.518033</td>\n",
       "      <td>0.532847</td>\n",
       "      <td>0.527710</td>\n",
       "      <td>0.511543</td>\n",
       "      <td>0.505929</td>\n",
       "      <td>0.505096</td>\n",
       "      <td>0.473268</td>\n",
       "      <td>0.500194</td>\n",
       "      <td>0.563858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462495</td>\n",
       "      <td>0.520767</td>\n",
       "      <td>0.514996</td>\n",
       "      <td>0.479704</td>\n",
       "      <td>0.547857</td>\n",
       "      <td>0.469282</td>\n",
       "      <td>0.477678</td>\n",
       "      <td>0.534086</td>\n",
       "      <td>0.459659</td>\n",
       "      <td>0.482997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.464015</td>\n",
       "      <td>0.547513</td>\n",
       "      <td>0.534929</td>\n",
       "      <td>0.515994</td>\n",
       "      <td>0.526294</td>\n",
       "      <td>0.519013</td>\n",
       "      <td>0.493227</td>\n",
       "      <td>0.487353</td>\n",
       "      <td>0.533328</td>\n",
       "      <td>0.515880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485028</td>\n",
       "      <td>0.492818</td>\n",
       "      <td>0.466966</td>\n",
       "      <td>0.514667</td>\n",
       "      <td>0.588129</td>\n",
       "      <td>0.480854</td>\n",
       "      <td>0.432877</td>\n",
       "      <td>0.501815</td>\n",
       "      <td>0.507247</td>\n",
       "      <td>0.502260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.464099</td>\n",
       "      <td>0.502493</td>\n",
       "      <td>0.530403</td>\n",
       "      <td>0.543423</td>\n",
       "      <td>0.488266</td>\n",
       "      <td>0.506906</td>\n",
       "      <td>0.496008</td>\n",
       "      <td>0.470768</td>\n",
       "      <td>0.488893</td>\n",
       "      <td>0.567098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449605</td>\n",
       "      <td>0.551441</td>\n",
       "      <td>0.525775</td>\n",
       "      <td>0.478567</td>\n",
       "      <td>0.537422</td>\n",
       "      <td>0.471184</td>\n",
       "      <td>0.465805</td>\n",
       "      <td>0.508096</td>\n",
       "      <td>0.466514</td>\n",
       "      <td>0.465507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451514</td>\n",
       "      <td>0.539376</td>\n",
       "      <td>0.544900</td>\n",
       "      <td>0.502148</td>\n",
       "      <td>0.503703</td>\n",
       "      <td>0.533397</td>\n",
       "      <td>0.448659</td>\n",
       "      <td>0.478863</td>\n",
       "      <td>0.508552</td>\n",
       "      <td>0.546285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482086</td>\n",
       "      <td>0.511703</td>\n",
       "      <td>0.477694</td>\n",
       "      <td>0.494675</td>\n",
       "      <td>0.542177</td>\n",
       "      <td>0.473592</td>\n",
       "      <td>0.467832</td>\n",
       "      <td>0.478610</td>\n",
       "      <td>0.443659</td>\n",
       "      <td>0.481261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.499596</td>\n",
       "      <td>0.503362</td>\n",
       "      <td>0.522893</td>\n",
       "      <td>0.496161</td>\n",
       "      <td>0.543045</td>\n",
       "      <td>0.461692</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.502430</td>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.537732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515923</td>\n",
       "      <td>0.500189</td>\n",
       "      <td>0.543489</td>\n",
       "      <td>0.511016</td>\n",
       "      <td>0.494478</td>\n",
       "      <td>0.466297</td>\n",
       "      <td>0.458757</td>\n",
       "      <td>0.538558</td>\n",
       "      <td>0.475912</td>\n",
       "      <td>0.485755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.497214</td>\n",
       "      <td>0.491898</td>\n",
       "      <td>0.538590</td>\n",
       "      <td>0.533611</td>\n",
       "      <td>0.515420</td>\n",
       "      <td>0.481304</td>\n",
       "      <td>0.514259</td>\n",
       "      <td>0.486543</td>\n",
       "      <td>0.492588</td>\n",
       "      <td>0.514609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538429</td>\n",
       "      <td>0.494881</td>\n",
       "      <td>0.531248</td>\n",
       "      <td>0.523854</td>\n",
       "      <td>0.466190</td>\n",
       "      <td>0.529339</td>\n",
       "      <td>0.471890</td>\n",
       "      <td>0.531397</td>\n",
       "      <td>0.437475</td>\n",
       "      <td>0.486962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.517050</td>\n",
       "      <td>0.514450</td>\n",
       "      <td>0.509608</td>\n",
       "      <td>0.503844</td>\n",
       "      <td>0.533506</td>\n",
       "      <td>0.505198</td>\n",
       "      <td>0.480254</td>\n",
       "      <td>0.489807</td>\n",
       "      <td>0.471894</td>\n",
       "      <td>0.506556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523795</td>\n",
       "      <td>0.467489</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.530422</td>\n",
       "      <td>0.487560</td>\n",
       "      <td>0.493201</td>\n",
       "      <td>0.484776</td>\n",
       "      <td>0.545881</td>\n",
       "      <td>0.466940</td>\n",
       "      <td>0.477744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.525592</td>\n",
       "      <td>0.497026</td>\n",
       "      <td>0.542631</td>\n",
       "      <td>0.507730</td>\n",
       "      <td>0.521060</td>\n",
       "      <td>0.475272</td>\n",
       "      <td>0.506918</td>\n",
       "      <td>0.474250</td>\n",
       "      <td>0.486505</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519751</td>\n",
       "      <td>0.510182</td>\n",
       "      <td>0.530916</td>\n",
       "      <td>0.509050</td>\n",
       "      <td>0.477800</td>\n",
       "      <td>0.491346</td>\n",
       "      <td>0.493666</td>\n",
       "      <td>0.532326</td>\n",
       "      <td>0.481168</td>\n",
       "      <td>0.488967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.521995</td>\n",
       "      <td>0.499324</td>\n",
       "      <td>0.531584</td>\n",
       "      <td>0.508733</td>\n",
       "      <td>0.551871</td>\n",
       "      <td>0.488382</td>\n",
       "      <td>0.494927</td>\n",
       "      <td>0.486490</td>\n",
       "      <td>0.486524</td>\n",
       "      <td>0.511331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525063</td>\n",
       "      <td>0.488777</td>\n",
       "      <td>0.522951</td>\n",
       "      <td>0.528720</td>\n",
       "      <td>0.499248</td>\n",
       "      <td>0.509396</td>\n",
       "      <td>0.497447</td>\n",
       "      <td>0.546639</td>\n",
       "      <td>0.460442</td>\n",
       "      <td>0.474039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.428787  0.525325  0.542139  0.478648  0.506824  0.490862  0.482278   \n",
       "1   0.444054  0.518033  0.532847  0.527710  0.511543  0.505929  0.505096   \n",
       "2   0.464015  0.547513  0.534929  0.515994  0.526294  0.519013  0.493227   \n",
       "3   0.464099  0.502493  0.530403  0.543423  0.488266  0.506906  0.496008   \n",
       "4   0.451514  0.539376  0.544900  0.502148  0.503703  0.533397  0.448659   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "77  0.499596  0.503362  0.522893  0.496161  0.543045  0.461692  0.509040   \n",
       "78  0.497214  0.491898  0.538590  0.533611  0.515420  0.481304  0.514259   \n",
       "79  0.517050  0.514450  0.509608  0.503844  0.533506  0.505198  0.480254   \n",
       "80  0.525592  0.497026  0.542631  0.507730  0.521060  0.475272  0.506918   \n",
       "81  0.521995  0.499324  0.531584  0.508733  0.551871  0.488382  0.494927   \n",
       "\n",
       "           7         8         9  ...       6.2       7.2       8.2       9.2  \\\n",
       "0   0.505213  0.516506  0.520210  ...  0.469042  0.494732  0.483015  0.492952   \n",
       "1   0.473268  0.500194  0.563858  ...  0.462495  0.520767  0.514996  0.479704   \n",
       "2   0.487353  0.533328  0.515880  ...  0.485028  0.492818  0.466966  0.514667   \n",
       "3   0.470768  0.488893  0.567098  ...  0.449605  0.551441  0.525775  0.478567   \n",
       "4   0.478863  0.508552  0.546285  ...  0.482086  0.511703  0.477694  0.494675   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "77  0.502430  0.476613  0.537732  ...  0.515923  0.500189  0.543489  0.511016   \n",
       "78  0.486543  0.492588  0.514609  ...  0.538429  0.494881  0.531248  0.523854   \n",
       "79  0.489807  0.471894  0.506556  ...  0.523795  0.467489  0.512300  0.530422   \n",
       "80  0.474250  0.486505  0.510403  ...  0.519751  0.510182  0.530916  0.509050   \n",
       "81  0.486490  0.486524  0.511331  ...  0.525063  0.488777  0.522951  0.528720   \n",
       "\n",
       "        10.2      11.2      12.2      13.2      14.2      15.2  \n",
       "0   0.539282  0.433638  0.487028  0.496327  0.451854  0.460499  \n",
       "1   0.547857  0.469282  0.477678  0.534086  0.459659  0.482997  \n",
       "2   0.588129  0.480854  0.432877  0.501815  0.507247  0.502260  \n",
       "3   0.537422  0.471184  0.465805  0.508096  0.466514  0.465507  \n",
       "4   0.542177  0.473592  0.467832  0.478610  0.443659  0.481261  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "77  0.494478  0.466297  0.458757  0.538558  0.475912  0.485755  \n",
       "78  0.466190  0.529339  0.471890  0.531397  0.437475  0.486962  \n",
       "79  0.487560  0.493201  0.484776  0.545881  0.466940  0.477744  \n",
       "80  0.477800  0.491346  0.493666  0.532326  0.481168  0.488967  \n",
       "81  0.499248  0.509396  0.497447  0.546639  0.460442  0.474039  \n",
       "\n",
       "[387 rows x 48 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.iloc[:,:-16]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3016ecb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cle_train_temp = cle_train.iloc[:,:-17]\n",
    "cle_train_Y = cle_train.iloc[:,-1]\n",
    "cle_train = pd.concat([cle_train_temp, cle_train_Y], axis=1)\n",
    "#cle_train.to_csv('cle_metadata_dnn_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0fd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_train_temp = vir_train.iloc[:,:-17]\n",
    "vir_train_Y = vir_train.iloc[:,-1]\n",
    "vir_train = pd.concat([vir_train_temp, vir_train_Y], axis=1)\n",
    "#vir_train.to_csv('vir_metadata_dnn_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ad66556",
   "metadata": {},
   "outputs": [],
   "source": [
    "hun_train_temp = hun_train.iloc[:,:-17]\n",
    "hun_train_Y = hun_train.iloc[:,-1]\n",
    "hun_train = pd.concat([hun_train_temp, hun_train_Y], axis=1)\n",
    "#hun_train.to_csv('hun_metadata_dnn_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e852ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_test_temp = cle_test.iloc[:,:-17]\n",
    "cle_test_Y = cle_test.iloc[:,-1]\n",
    "cle_test = pd.concat([cle_test_temp, cle_test_Y], axis=1)\n",
    "#cle_test.to_csv('cle_metadata_dnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9041c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vir_test_temp = vir_test.iloc[:,:-17]\n",
    "vir_test_Y = vir_test.iloc[:,-1]\n",
    "vir_test = pd.concat([vir_test_temp, vir_test_Y], axis=1)\n",
    "#vir_test.to_csv('vir_metadata_dnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4764f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hun_test_temp = hun_test.iloc[:,:-17]\n",
    "hun_test_Y = hun_test.iloc[:,-1]\n",
    "hun_test = pd.concat([hun_test_temp, hun_test_Y], axis=1)\n",
    "#hun_test.to_csv('hun_metadata_dnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4349043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>7.3</th>\n",
       "      <th>8.3</th>\n",
       "      <th>9.3</th>\n",
       "      <th>10.3</th>\n",
       "      <th>11.3</th>\n",
       "      <th>12.3</th>\n",
       "      <th>13.3</th>\n",
       "      <th>14.3</th>\n",
       "      <th>15.3</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.522762</td>\n",
       "      <td>0.504038</td>\n",
       "      <td>0.527250</td>\n",
       "      <td>0.498387</td>\n",
       "      <td>0.565987</td>\n",
       "      <td>0.509586</td>\n",
       "      <td>0.479623</td>\n",
       "      <td>0.509473</td>\n",
       "      <td>0.480492</td>\n",
       "      <td>0.534721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537556</td>\n",
       "      <td>0.453480</td>\n",
       "      <td>0.577583</td>\n",
       "      <td>0.446029</td>\n",
       "      <td>0.485879</td>\n",
       "      <td>0.386280</td>\n",
       "      <td>0.497379</td>\n",
       "      <td>0.454557</td>\n",
       "      <td>0.509074</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512266</td>\n",
       "      <td>0.496535</td>\n",
       "      <td>0.537593</td>\n",
       "      <td>0.512366</td>\n",
       "      <td>0.549310</td>\n",
       "      <td>0.491441</td>\n",
       "      <td>0.498024</td>\n",
       "      <td>0.484337</td>\n",
       "      <td>0.487093</td>\n",
       "      <td>0.521420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519116</td>\n",
       "      <td>0.457791</td>\n",
       "      <td>0.554603</td>\n",
       "      <td>0.459480</td>\n",
       "      <td>0.484268</td>\n",
       "      <td>0.438276</td>\n",
       "      <td>0.499722</td>\n",
       "      <td>0.489912</td>\n",
       "      <td>0.514547</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.491952</td>\n",
       "      <td>0.524946</td>\n",
       "      <td>0.485390</td>\n",
       "      <td>0.539485</td>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.503719</td>\n",
       "      <td>0.480027</td>\n",
       "      <td>0.475724</td>\n",
       "      <td>0.530794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.544467</td>\n",
       "      <td>0.473762</td>\n",
       "      <td>0.562892</td>\n",
       "      <td>0.473715</td>\n",
       "      <td>0.511834</td>\n",
       "      <td>0.417594</td>\n",
       "      <td>0.485931</td>\n",
       "      <td>0.456725</td>\n",
       "      <td>0.503743</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.495323</td>\n",
       "      <td>0.521070</td>\n",
       "      <td>0.486139</td>\n",
       "      <td>0.526061</td>\n",
       "      <td>0.464749</td>\n",
       "      <td>0.493206</td>\n",
       "      <td>0.486621</td>\n",
       "      <td>0.492749</td>\n",
       "      <td>0.515017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515474</td>\n",
       "      <td>0.460602</td>\n",
       "      <td>0.541977</td>\n",
       "      <td>0.508290</td>\n",
       "      <td>0.469738</td>\n",
       "      <td>0.478705</td>\n",
       "      <td>0.516899</td>\n",
       "      <td>0.476888</td>\n",
       "      <td>0.537910</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.504315</td>\n",
       "      <td>0.504432</td>\n",
       "      <td>0.526749</td>\n",
       "      <td>0.493828</td>\n",
       "      <td>0.539291</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.504622</td>\n",
       "      <td>0.498895</td>\n",
       "      <td>0.482281</td>\n",
       "      <td>0.543537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527635</td>\n",
       "      <td>0.454373</td>\n",
       "      <td>0.578429</td>\n",
       "      <td>0.465488</td>\n",
       "      <td>0.502486</td>\n",
       "      <td>0.381890</td>\n",
       "      <td>0.476488</td>\n",
       "      <td>0.486014</td>\n",
       "      <td>0.542625</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.521899</td>\n",
       "      <td>0.506514</td>\n",
       "      <td>0.491817</td>\n",
       "      <td>0.525493</td>\n",
       "      <td>0.516104</td>\n",
       "      <td>0.489461</td>\n",
       "      <td>0.497187</td>\n",
       "      <td>0.504617</td>\n",
       "      <td>0.488040</td>\n",
       "      <td>0.539020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529264</td>\n",
       "      <td>0.474164</td>\n",
       "      <td>0.539387</td>\n",
       "      <td>0.495350</td>\n",
       "      <td>0.491914</td>\n",
       "      <td>0.456760</td>\n",
       "      <td>0.488486</td>\n",
       "      <td>0.488896</td>\n",
       "      <td>0.543096</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.493402</td>\n",
       "      <td>0.513794</td>\n",
       "      <td>0.503867</td>\n",
       "      <td>0.505538</td>\n",
       "      <td>0.537071</td>\n",
       "      <td>0.468497</td>\n",
       "      <td>0.501576</td>\n",
       "      <td>0.509287</td>\n",
       "      <td>0.484469</td>\n",
       "      <td>0.535978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526331</td>\n",
       "      <td>0.445463</td>\n",
       "      <td>0.563322</td>\n",
       "      <td>0.469648</td>\n",
       "      <td>0.498098</td>\n",
       "      <td>0.386164</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.466991</td>\n",
       "      <td>0.551914</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.497897</td>\n",
       "      <td>0.508730</td>\n",
       "      <td>0.510682</td>\n",
       "      <td>0.498494</td>\n",
       "      <td>0.547744</td>\n",
       "      <td>0.476758</td>\n",
       "      <td>0.501776</td>\n",
       "      <td>0.500636</td>\n",
       "      <td>0.481379</td>\n",
       "      <td>0.542525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535152</td>\n",
       "      <td>0.447812</td>\n",
       "      <td>0.603077</td>\n",
       "      <td>0.465198</td>\n",
       "      <td>0.499517</td>\n",
       "      <td>0.383318</td>\n",
       "      <td>0.460699</td>\n",
       "      <td>0.462050</td>\n",
       "      <td>0.546613</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.506661</td>\n",
       "      <td>0.498455</td>\n",
       "      <td>0.525077</td>\n",
       "      <td>0.517549</td>\n",
       "      <td>0.547720</td>\n",
       "      <td>0.491332</td>\n",
       "      <td>0.483464</td>\n",
       "      <td>0.487521</td>\n",
       "      <td>0.464264</td>\n",
       "      <td>0.503385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534532</td>\n",
       "      <td>0.481953</td>\n",
       "      <td>0.585833</td>\n",
       "      <td>0.481229</td>\n",
       "      <td>0.507291</td>\n",
       "      <td>0.407615</td>\n",
       "      <td>0.471929</td>\n",
       "      <td>0.495463</td>\n",
       "      <td>0.526446</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.517776</td>\n",
       "      <td>0.477103</td>\n",
       "      <td>0.549429</td>\n",
       "      <td>0.512531</td>\n",
       "      <td>0.520010</td>\n",
       "      <td>0.488564</td>\n",
       "      <td>0.503612</td>\n",
       "      <td>0.492379</td>\n",
       "      <td>0.505658</td>\n",
       "      <td>0.513014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537621</td>\n",
       "      <td>0.451623</td>\n",
       "      <td>0.564879</td>\n",
       "      <td>0.484598</td>\n",
       "      <td>0.487385</td>\n",
       "      <td>0.464928</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.479352</td>\n",
       "      <td>0.531629</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.522762  0.504038  0.527250  0.498387  0.565987  0.509586  0.479623   \n",
       "1   0.512266  0.496535  0.537593  0.512366  0.549310  0.491441  0.498024   \n",
       "2   0.529690  0.491952  0.524946  0.485390  0.539485  0.485555  0.503719   \n",
       "3   0.515700  0.495323  0.521070  0.486139  0.526061  0.464749  0.493206   \n",
       "4   0.504315  0.504432  0.526749  0.493828  0.539291  0.465600  0.504622   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "36  0.521899  0.506514  0.491817  0.525493  0.516104  0.489461  0.497187   \n",
       "37  0.493402  0.513794  0.503867  0.505538  0.537071  0.468497  0.501576   \n",
       "38  0.497897  0.508730  0.510682  0.498494  0.547744  0.476758  0.501776   \n",
       "39  0.506661  0.498455  0.525077  0.517549  0.547720  0.491332  0.483464   \n",
       "40  0.517776  0.477103  0.549429  0.512531  0.520010  0.488564  0.503612   \n",
       "\n",
       "           7         8         9  ...       7.3       8.3       9.3      10.3  \\\n",
       "0   0.509473  0.480492  0.534721  ...  0.537556  0.453480  0.577583  0.446029   \n",
       "1   0.484337  0.487093  0.521420  ...  0.519116  0.457791  0.554603  0.459480   \n",
       "2   0.480027  0.475724  0.530794  ...  0.544467  0.473762  0.562892  0.473715   \n",
       "3   0.486621  0.492749  0.515017  ...  0.515474  0.460602  0.541977  0.508290   \n",
       "4   0.498895  0.482281  0.543537  ...  0.527635  0.454373  0.578429  0.465488   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "36  0.504617  0.488040  0.539020  ...  0.529264  0.474164  0.539387  0.495350   \n",
       "37  0.509287  0.484469  0.535978  ...  0.526331  0.445463  0.563322  0.469648   \n",
       "38  0.500636  0.481379  0.542525  ...  0.535152  0.447812  0.603077  0.465198   \n",
       "39  0.487521  0.464264  0.503385  ...  0.534532  0.481953  0.585833  0.481229   \n",
       "40  0.492379  0.505658  0.513014  ...  0.537621  0.451623  0.564879  0.484598   \n",
       "\n",
       "        11.3      12.3      13.3      14.3      15.3   num  \n",
       "0   0.485879  0.386280  0.497379  0.454557  0.509074  0.75  \n",
       "1   0.484268  0.438276  0.499722  0.489912  0.514547  0.75  \n",
       "2   0.511834  0.417594  0.485931  0.456725  0.503743  0.50  \n",
       "3   0.469738  0.478705  0.516899  0.476888  0.537910  0.25  \n",
       "4   0.502486  0.381890  0.476488  0.486014  0.542625  0.25  \n",
       "..       ...       ...       ...       ...       ...   ...  \n",
       "36  0.491914  0.456760  0.488486  0.488896  0.543096  0.25  \n",
       "37  0.498098  0.386164  0.491525  0.466991  0.551914  0.25  \n",
       "38  0.499517  0.383318  0.460699  0.462050  0.546613  0.75  \n",
       "39  0.507291  0.407615  0.471929  0.495463  0.526446  0.25  \n",
       "40  0.487385  0.464928  0.472260  0.479352  0.531629  0.75  \n",
       "\n",
       "[123 rows x 65 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swi_test_new = pd.concat([swi_train, swi_test])\n",
    "swi_test_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd79e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "swi_test_temp = swi_test_new.iloc[:,:-17]\n",
    "swi_test_Y = swi_test_new.iloc[:,-1]\n",
    "swi_test = pd.concat([swi_test_temp, swi_test_Y], axis=1)\n",
    "#swi_test.to_csv('swi_metadata_dnn_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a9da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for deep learning testing\n",
    "def Test(path_train,path_test,model_name):\n",
    "    Train = pd.read_csv(path_train)\n",
    "    Test = pd.read_csv(path_test)\n",
    "    \n",
    "    X_train = Train.iloc[:,:-1]\n",
    "    Y_train = Train.iloc[:,-1]\n",
    "    \n",
    "    X_test = Test.iloc[:,:-1]\n",
    "    Y_test = Test.iloc[:,-1]\n",
    "\n",
    "    #binarize the target\n",
    "    Y_train_binary = Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "    Y_test_binary = Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "\n",
    "    Y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    \n",
    "    cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "    print(cm)\n",
    "    print(classification_report(Y_test_binary, Y_pred, digits=4))\n",
    "    \n",
    "    mismatch = [i for i, (a,b) in enumerate(zip(Y_pred, Y_test_binary)) if a != b]\n",
    "    print(mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e623e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86ca29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(48,1)))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.fit(X_train, Y_train_binary, epochs=1000, batch_size=32,callbacks=[callback])\n",
    "Y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    \n",
    "cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "print(cm)\n",
    "print(classification_report(Y_test_binary, Y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/Meta_Only/DNNMeta_CNN_test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1bf045",
   "metadata": {},
   "source": [
    "# Test on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_dnn_train.csv'\n",
    "path_test = 'cle_metadata_dnn_test.csv'\n",
    "model = '../Models/Meta_Only/DNNMeta_CNN_test.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_dnn_train.csv'\n",
    "path_test = 'vir_metadata_dnn_test.csv'\n",
    "model = '../Models/Meta_Only/DNNMeta_CNN_test.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_dnn_train.csv'\n",
    "path_test = 'hun_metadata_dnn_test.csv'\n",
    "model = '../Models/Meta_Only/DNNMeta_CNN_test.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'cle_metadata_dnn_train.csv'\n",
    "path_test = 'swi_metadata_dnn_test.csv'\n",
    "model = '../Models/Meta_Only/DNNMeta_CNN_test.h5'\n",
    "Test(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ee9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779d148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fff2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = \n",
    "X_test = \n",
    "Y_train_binary = \n",
    "Y_test_binary = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0813f",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fddd185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "17/17 [==============================] - 1s 3ms/step - loss: 0.6950 - accuracy: 0.4934\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6938 - accuracy: 0.4991\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.5366\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.5347\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6836 - accuracy: 0.5591\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.5779\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.5910\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.5966\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6605 - accuracy: 0.6191\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6472 - accuracy: 0.6266\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.6379\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6467 - accuracy: 0.6098\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6283 - accuracy: 0.6341\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.6529\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6093 - accuracy: 0.6604\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6285\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6040 - accuracy: 0.6923\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6301 - accuracy: 0.6210\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6230 - accuracy: 0.6735\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5939 - accuracy: 0.7111\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5814 - accuracy: 0.6886\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7073\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.7017\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6360 - accuracy: 0.6154\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.7054\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6108 - accuracy: 0.6473\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5922 - accuracy: 0.6904\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7092\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7430\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5556 - accuracy: 0.7223\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.7129\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7242\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.7167\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5636 - accuracy: 0.7111\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "[[ 31  66]\n",
      " [104 186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3196    0.2296    0.2672       135\n",
      "           1     0.6414    0.7381    0.6863       252\n",
      "\n",
      "    accuracy                         0.5607       387\n",
      "   macro avg     0.4805    0.4839    0.4768       387\n",
      "weighted avg     0.5291    0.5607    0.5401       387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(48,), activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.fit(X_train, Y_train_binary, epochs=1000, batch_size=32,callbacks=[callback])\n",
    "Y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    \n",
    "cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "print(cm)\n",
    "print(classification_report(Y_test_binary, Y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/Meta_Only/CNNMeta_DNN_dropswi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89a844",
   "metadata": {},
   "source": [
    "# test on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_DNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_DNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_DNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_DNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12522ce3",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "880e0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e78c2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/18 [==============================] - 4s 31ms/step - loss: 0.6837 - accuracy: 0.5511\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.6725 - accuracy: 0.5566\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.6516 - accuracy: 0.6004\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.6282 - accuracy: 0.6697\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.7023 - accuracy: 0.5347\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.6890 - accuracy: 0.5328\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.6684 - accuracy: 0.5949\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.6395 - accuracy: 0.6606\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.5876 - accuracy: 0.7153\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.5735 - accuracy: 0.6971\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.5532 - accuracy: 0.6934\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.5332 - accuracy: 0.7427\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.5512 - accuracy: 0.7226\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.4935 - accuracy: 0.7719\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.4789 - accuracy: 0.7792\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.5473 - accuracy: 0.7591\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.5308 - accuracy: 0.7591\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.5130 - accuracy: 0.7664\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.5140 - accuracy: 0.7573\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.5298 - accuracy: 0.7409\n",
      "[[ 98 224]\n",
      " [ 30 102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3043    0.7656    0.4356       128\n",
      "           1     0.7727    0.3129    0.4454       326\n",
      "\n",
      "    accuracy                         0.4405       454\n",
      "   macro avg     0.5385    0.5393    0.4405       454\n",
      "weighted avg     0.6407    0.4405    0.4426       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=64, return_sequences=True, input_shape=(48, 1)))\n",
    "model.add(SimpleRNN(units=32, return_sequences=True))\n",
    "model.add(SimpleRNN(units=16))\n",
    "model.add(Dense(units=2, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "model.fit(X_train, Y_train_binary, epochs=1000, batch_size=32,callbacks=[callback])\n",
    "Y_pred = model.predict(X_test).argmax(axis=1)\n",
    "    \n",
    "cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "print(cm)\n",
    "print(classification_report(Y_test_binary, Y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedaab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../Models/Meta_Only/CNNMeta_RNN_dropswi.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fae7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_RNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_RNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_RNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'swi_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_Only/CNNMeta_RNN_dropswi.h5'\n",
    "Test(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812110c",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bde29",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61dc493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2175bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is:\n",
      "[[ 82  43]\n",
      " [ 46 283]]\n",
      "Accuracy is : 0.8039647577092511\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       128\n",
      "           1       0.86      0.87      0.86       326\n",
      "\n",
      "    accuracy                           0.80       454\n",
      "   macro avg       0.76      0.75      0.76       454\n",
      "weighted avg       0.80      0.80      0.80       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X_train, Y_train_binary)\n",
    "Y_predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(Y_predictions, Y_test_binary)\n",
    "print(\"Confusion Matrix is:\")\n",
    "print(cm)\n",
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements\n",
    "print(\"Accuracy is : \" + str(accuracy(cm)))\n",
    "    \n",
    "print(\"Report\")\n",
    "print(classification_report(Y_test_binary, Y_predictions))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d529c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save clf model\n",
    "from joblib import dump, load\n",
    "dump(clf, '../Models/Meta_only/CNNMeta_dt.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08cf24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Decision tree, and random forest testing\n",
    "def Test_DT(path_train,path_test,model_name):\n",
    "    Train = pd.read_csv(path_train)\n",
    "    Test = pd.read_csv(path_test)\n",
    "    \n",
    "    X_train = Train.iloc[:,:-1]\n",
    "    Y_train = Train.iloc[:,-1]\n",
    "\n",
    "    X_test = Test.iloc[:,:-1]\n",
    "    Y_test = Test.iloc[:,-1]\n",
    "\n",
    "    #binarize the target\n",
    "    Y_train_binary = Y_train.apply(lambda x: 1 if x > 0 else 0)\n",
    "    Y_test_binary = Y_test.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    model = load(model_name)\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "    print(cm)\n",
    "    print(classification_report(Y_test_binary, Y_pred, digits=4))\n",
    "    \n",
    "    mismatch = [i for i, (a,b) in enumerate(zip(Y_pred, Y_test_binary)) if a != b]\n",
    "    print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on each dataset for decision tree\n",
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_dt.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_dt.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_dt.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_dt.joblib'\n",
    "Test_DT(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f058a8",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dee5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e9459fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103 185]\n",
      " [ 25 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3576    0.8047    0.4952       128\n",
      "           1     0.8494    0.4325    0.5732       326\n",
      "\n",
      "    accuracy                         0.5374       454\n",
      "   macro avg     0.6035    0.6186    0.5342       454\n",
      "weighted avg     0.7108    0.5374    0.5512       454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, Y_train_binary)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "print(cm)\n",
    "print(classification_report(Y_test_binary, Y_pred,digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54a50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(classifier, '../Models/Meta_only/CNNMeta_rf_9005.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49633d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on each dataset for Random Forest\n",
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_rf_9005.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_rf_9005.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_rf_9005.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_rf_9005.joblib'\n",
    "Test_DT(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a511e046",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb18996",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train_binary.values.ravel())\n",
    "y_pred = svc.predict(X_test)\n",
    "print(confusion_matrix(Y_test_binary, y_pred))\n",
    "print(classification_report(Y_test_binary, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(svc, \"../Models/Meta_only/CNNMeta_svm_8978.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on each dataset for SVM\n",
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_svm_8978.pkl'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_svm_8978.pkl'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_svm_8978.pkl'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_svm_8978.pkl'\n",
    "Test_DT(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26593df9",
   "metadata": {},
   "source": [
    "# Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, Y_train_binary)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(Y_pred, Y_test_binary)\n",
    "print(cm)\n",
    "print(classification_report(Y_test_binary, Y_pred, digits = 4))\n",
    "\n",
    "dump(clf, '../Models/Meta_only/CNNMeta_NB_dropswi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cle test\")\n",
    "path_train = 'cle_metadata_cnn_train.csv'\n",
    "path_test = 'cle_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_NB_dropswi.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"vir test\")\n",
    "path_train = 'vir_metadata_cnn_train.csv'\n",
    "path_test = 'vir_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_NB_dropswi.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"hun test\")\n",
    "path_train = 'hun_metadata_cnn_train.csv'\n",
    "path_test = 'hun_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_NB_dropswi.joblib'\n",
    "Test_DT(path_train,path_test,model)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"swi test\")\n",
    "path_train = 'swi_metadata_cnn_train.csv'\n",
    "path_test = 'swi_metadata_cnn_test.csv'\n",
    "model = '../Models/Meta_only/CNNMeta_NB_dropswi.joblib'\n",
    "Test_DT(path_train,path_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2670c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
